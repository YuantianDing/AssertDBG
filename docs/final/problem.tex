
\section{Motivation}

\subsection{Problem Definition}

I follow the problem definition of code generation in \cite{TeachSelfDebug}. Basically, each sample
can be represented as a triplet $(Q, T_v, T_h)$ where $Q$ is the natural language description of the task, $T_v$ is the target code, and $T_h$ is the code generated by the model. $Q$ may include a description of the task, the signature of the function to be implemented, and the constraints that the code must satisfy. In standard code generation tasks, both $Q$ and $T_v$ are provided as inputs, and the objective is to generate a program $P$ that meets the specifications of both $T_v$ and $T_h$. The generated program $P$ is evaluated against both $T_v$ and $T_h$ to check its correctness.

In this project, I specifically aim to improve the correctness of code generated by large language models (LLMs). Achieving correctness is particularly challenging for LLMs because, despite their impressive fluency and syntactic capabilities, they often lack a deep semantic understanding of the tasks they are solving. As a result, LLMs can produce code that appears syntactically valid and may even pass a limited set of test cases, yet still fail to fully capture the problem's intent or handle critical edge cases.

To address this issue, I explore multi-agent collaboration-based testing methods, which enhance reliability by introducing diverse perspectives into the validation process. However, LLMs frequently struggle to generate robust and accurate test cases, primarily due to a lack of sufficient task-specific data or reasoning depth.

To illustrate this limitation, I randomly selected one benchmark problem \texttt{HumanEval/118} from the HumanEvalPlus benchmark suite. Using the \texttt{gpt-4o} model, I generated a corresponding test function. As shown in Figure~\ref{fig:program}, although the model is capable of generating a correct solution for this problem, it fails to produce valid test cases. 

In this project, I propose assertion-based debugging to address this challenge. By embedding runtime assertions throughout the code, I can expose hidden errors that might be overlooked by surface-level test cases. These assertions act as lightweight contracts that enforce input-output constraints, loop invariants, and other correctness properties during execution. When an assertion fails, it provides a precise signal to a debugger agent, which then locates and fixes the faulty function in an iterative loop. This structured approach mimics human debugging strategies and enhances both the reliability and interpretability of LLM-generated code.


\begin{figure*}[t]
\begin{python}
def get_closest_vowel(word):
    """You are given a word. Your task is to find the closest vowel that stands between 
    two consonants from the right side of the word (case sensitive).
    
    Vowels in the beginning and ending doesn't count. Return empty string if you didn't
    find any vowel met the above condition. 

    You may assume that the given string contains English letter only.

    Example:
    get_closest_vowel("yogurt") ==> "u"
    get_closest_vowel("FULL") ==> "U"
    get_closest_vowel("quick") ==> ""
    get_closest_vowel("ab") ==> ""
    """
    def is_vowel(ch: str) -> bool:
        return ch in "aeiouAEIOU"
    for i in range(len(word) - 2, 0, -1):
        if is_vowel(word[i]) and not is_vowel(word[i-1]) and not is_vowel(word[i+1]):
            return word[i]
    return ""

def test_get_closest_vowel():
    assert get_closest_vowel("yogurt") == "u"
    assert get_closest_vowel("FULL") == "U"
    assert get_closest_vowel("planet") == "e"
    assert get_closest_vowel("ab") == ""
    assert get_closest_vowel("HeLLo") == ""  # INCORRECT !!!
    assert get_closest_vowel("ABcdEF") == "" # INCORRECT !!!
    assert get_closest_vowel("a") == ""
    ......

\end{python}
\caption{Example of incorrect test cases generated by \texttt{gpt-4o} while the program can be correctly generated. }
\label{fig:program}
\end{figure*}


